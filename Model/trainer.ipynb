{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9709f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a9dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run configs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0267cc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100, 66)\n",
      "Model: \"ASR_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 52)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, None, 50)             39050     ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, None, 50)             200       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " p_re_lu (PReLU)             (None, None, 50)             50        ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, None, 50)             37550     ['p_re_lu[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, None, 50)             200       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_1 (PReLU)           (None, None, 50)             50        ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, None, 50)             37550     ['p_re_lu_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, None, 50)             200       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_2 (PReLU)           (None, None, 50)             50        ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, 50)             0         ['p_re_lu[0][0]',             \n",
      "                                                                     'p_re_lu_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, None, 50)             37550     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, None, 50)             200       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_3 (PReLU)           (None, None, 50)             50        ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, None, 50)             37550     ['p_re_lu_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, None, 50)             200       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_4 (PReLU)           (None, None, 50)             50        ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, None, 50)             0         ['add[0][0]',                 \n",
      "                                                                     'p_re_lu_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, None, 50)             37550     ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, None, 50)             200       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_5 (PReLU)           (None, None, 50)             50        ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, None, 50)             37550     ['p_re_lu_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, None, 50)             200       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_6 (PReLU)           (None, None, 50)             50        ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, None, 50)             0         ['add_1[0][0]',               \n",
      "                                                                     'p_re_lu_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, None, 50)             37550     ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, None, 50)             200       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_7 (PReLU)           (None, None, 50)             50        ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, None, 50)             37550     ['p_re_lu_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, None, 50)             200       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_8 (PReLU)           (None, None, 50)             50        ['batch_normalization_8[0][0]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, None, 50)             0         ['add_2[0][0]',               \n",
      "                                                                     'p_re_lu_8[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, None, 50)             37550     ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, None, 50)             200       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " p_re_lu_9 (PReLU)           (None, None, 50)             50        ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, None, 50)             37550     ['p_re_lu_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, None, 50)             200       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " p_re_lu_10 (PReLU)          (None, None, 50)             50        ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, None, 50)             0         ['add_3[0][0]',               \n",
      "                                                                     'p_re_lu_10[0][0]']          \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, None, 340)            300560    ['add_4[0][0]']               \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, None, 340)            694960    ['bidirectional[0][0]']       \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 340)            115940    ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, None, 340)            0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 66)             22506     ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)  (None, None, 66)             0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1551266 (5.92 MB)\n",
      "Trainable params: 1550166 (5.91 MB)\n",
      "Non-trainable params: 1100 (4.30 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ea7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import edit_distance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53057dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_wavs, train_texts, test_wavs, test_texts, epochs=100, batch_size=50):\n",
    "\n",
    "    with tf.device(device_name):\n",
    "\n",
    "        for e in range(0, epochs + epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            len_train = len(train_wavs)\n",
    "            len_test = len(test_wavs)\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            test_CER = 0\n",
    "            train_batch_count = 0\n",
    "            test_batch_count = 0\n",
    "\n",
    "            print(\"Training epoch: {}\".format(e+1))\n",
    "            for start in tqdm(range(0, len_train, batch_size)):\n",
    "\n",
    "                end = None\n",
    "                if start + batch_size < len_train:\n",
    "                    end = start + batch_size\n",
    "                else:\n",
    "                    end = len_train\n",
    "                x, target, target_lengths, output_lengths = batchify(\n",
    "                    train_wavs[start:end], train_texts[start:end], UNQ_CHARS)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = model(x, training=True)\n",
    "\n",
    "                    loss = K.ctc_batch_cost(\n",
    "                        target, output, output_lengths, target_lengths)\n",
    "\n",
    "                grads = tape.gradient(loss, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "                train_loss += np.average(loss.numpy())\n",
    "                train_batch_count += 1\n",
    "\n",
    "            print(\"Testing epoch: {}\".format(e+1))\n",
    "            for start in tqdm(range(0, len_test, batch_size)):\n",
    "\n",
    "                end = None\n",
    "                if start + batch_size < len_test:\n",
    "                    end = start + batch_size\n",
    "                else:\n",
    "                    end = len_test\n",
    "                x, target, target_lengths, output_lengths = batchify(\n",
    "                    test_wavs[start:end], test_texts[start:end], UNQ_CHARS)\n",
    "\n",
    "                output = model(x, training=False)\n",
    "\n",
    "                # Calculate CTC Loss\n",
    "                loss = K.ctc_batch_cost(\n",
    "                    target, output, output_lengths, target_lengths)\n",
    "\n",
    "                test_loss += np.average(loss.numpy())\n",
    "                test_batch_count += 1\n",
    "\n",
    "                \"\"\"\n",
    "                    The line of codes below is for computing evaluation metric (CER) on internal validation data.\n",
    "                \"\"\"\n",
    "                input_len = np.ones(output.shape[0]) * output.shape[1]\n",
    "                decoded_indices = K.ctc_decode(output, input_length=input_len,\n",
    "                                       greedy=False, beam_width=100)[0][0]\n",
    "                \n",
    "                # Remove the padding token from batchified target texts\n",
    "                target_indices = [sent[sent != 0].tolist() for sent in target]\n",
    "\n",
    "                # Remove the padding, unknown token, and blank token from predicted texts\n",
    "                predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices] # idx 0: padding token, idx 1: unknown, idx -1: blank token\n",
    "\n",
    "                len_batch = end - start\n",
    "                for i in range(len_batch):\n",
    "\n",
    "                    pred = predicted_indices[i]\n",
    "                    truth = target_indices[i]\n",
    "                    sm = ed.SequenceMatcher(pred, truth)\n",
    "                    ed_dist = sm.distance()                 # Edit distance\n",
    "                    test_CER += ed_dist / len(truth)\n",
    "                test_CER /= len_batch\n",
    "\n",
    "            train_loss /= train_batch_count\n",
    "            test_loss /= test_batch_count\n",
    "            test_CER /= test_batch_count\n",
    "\n",
    "            rec = \"Epoch: {}, Train Loss: {:.2f}, Test Loss {:.2f}, Test CER {:.2f} % in {:.2f} secs.\\n\".format(\n",
    "                e+1, train_loss, test_loss, test_CER*100, time.time() - start_time)\n",
    "\n",
    "            print(rec)\n",
    "            #Save the final trained model\n",
    "            model.save(\"model/trained_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f43c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(wavs_dir, texts_dir):\n",
    "    texts_df = pd.read_csv(texts_dir)\n",
    "    train_wavs = []\n",
    "    for f_name in texts_df[\"file\"]:\n",
    "        wav, _ = librosa.load(f\"{wavs_dir}/{f_name}.flac\", sr=SR)\n",
    "        train_wavs.append(wav)\n",
    "    train_texts = texts_df[\"text\"].tolist()\n",
    "    return train_wavs, train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db2ff6",
   "metadata": {},
   "source": [
    "# Defintion of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b7bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined ✅ ✅ ✅ ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model(INPUT_DIM, NUM_UNQ_CHARS, num_res_blocks=5, num_cnn_layers=2,\n",
    "                      cnn_filters=50, cnn_kernel_size=15, rnn_dim=170, rnn_dropout=0.15, num_rnn_layers=2,\n",
    "                      num_dense_layers=1, dense_dim=340, model_name=MODEL_NAME, rnn_type=\"lstm\",\n",
    "                      use_birnn=True)\n",
    "print(\"Model defined \\u2705 \\u2705 \\u2705 \\u2705\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b62ebd",
   "metadata": {},
   "source": [
    "# Defintion of the optimizer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b770272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.adam.Adam at 0x209ace7efe0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00cc3b",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c95ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.....\n",
      "Data loaded ✅ ✅ ✅ ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data.....\")\n",
    "train_wavs, train_texts = load_data(wavs_dir=\"dataset/wav_files(sampled)\", \n",
    "                                    texts_dir=\"dataset/transcriptions(sampled)/file_speaker_text(sampled).csv\")\n",
    "print(\"Data loaded \\u2705 \\u2705 \\u2705 \\u2705\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c9ad0",
   "metadata": {},
   "source": [
    "### To replicate the results give the argument astext_dir=\"dataset/transcriptions(sampled)/file_speaker_text(orignally_trained).csv\".\n",
    "### Get all of the wavs files from https://openslr.org/54/, put them in a single directory, and give that directory as argument for wavs_dir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca665b45",
   "metadata": {},
   "source": [
    "# Clean the audio file by removing the silent gaps from the both ends the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2477034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the audio files.....\n",
      "Audio files cleaned ✅ ✅ ✅ ✅\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.8005371e-03,  2.3193359e-03,  2.4414062e-03, ...,\n",
       "        -5.7573766e-06, -5.7573766e-06, -5.7573766e-06], dtype=float32),\n",
       " array([ 3.2196045e-02,  4.5593262e-02,  5.3833008e-02, ...,\n",
       "        -5.0090207e-06, -5.0090207e-06, -5.0090207e-06], dtype=float32),\n",
       " array([-5.7983398e-04,  7.6293945e-04,  6.1035156e-05, ...,\n",
       "        -5.9117820e-06, -5.9117820e-06, -5.9117820e-06], dtype=float32),\n",
       " array([-2.6733398e-02, -2.7770996e-02, -2.9327393e-02, ...,\n",
       "        -4.3437790e-06, -4.3437790e-06, -4.3437790e-06], dtype=float32),\n",
       " array([-2.5299072e-02, -2.4932861e-02, -2.3712158e-02, ...,\n",
       "        -4.4921876e-06, -4.4921876e-06, -4.4921876e-06], dtype=float32),\n",
       " array([-3.3508301e-02, -2.5238037e-02, -2.2949219e-02, ...,\n",
       "         1.4448364e-06,  1.4448364e-06,  1.4448364e-06], dtype=float32),\n",
       " array([-1.0284424e-02, -1.4831543e-02, -1.9409180e-02, ...,\n",
       "        -2.2604054e-05, -2.2604054e-05, -2.2604054e-05], dtype=float32),\n",
       " array([-5.3924561e-02,  1.4953613e-02,  2.4749756e-02, ...,\n",
       "        -3.3317476e-05, -3.3317476e-05, -3.3317476e-05], dtype=float32),\n",
       " array([ 1.1126709e-01, -1.2145996e-01,  5.0384521e-02, ...,\n",
       "         1.5994285e-06,  1.5994285e-06,  1.5994285e-06], dtype=float32),\n",
       " array([ 3.7231445e-03,  3.0517578e-03,  3.6621094e-04, ...,\n",
       "        -2.6568576e-05, -2.6568576e-05, -2.6568576e-05], dtype=float32),\n",
       " array([ 2.3284912e-02,  1.9256592e-02,  1.5014648e-02, ...,\n",
       "        -7.2273447e-06, -7.2273447e-06, -7.2273447e-06], dtype=float32),\n",
       " array([-1.3549805e-02, -1.2420654e-02, -1.0314941e-02, ...,\n",
       "        -8.6787713e-06, -8.6787713e-06, -8.6787713e-06], dtype=float32),\n",
       " array([-1.2817383e-03,  3.8757324e-03,  3.6926270e-03, ...,\n",
       "        -8.8440547e-06, -8.8440547e-06, -8.8440547e-06], dtype=float32),\n",
       " array([-5.1879883e-04,  5.4931641e-04,  3.3569336e-04, ...,\n",
       "        -7.9186484e-06, -7.9186484e-06, -7.9186484e-06], dtype=float32),\n",
       " array([ 2.8076172e-03, -5.7983398e-04,  1.4953613e-03, ...,\n",
       "        -5.7856241e-06, -5.7856241e-06, -5.7856241e-06], dtype=float32),\n",
       " array([ 8.6669922e-03, -4.7302246e-03, -2.2277832e-03, ...,\n",
       "        -1.1136952e-05, -1.1136952e-05, -1.1136952e-05], dtype=float32),\n",
       " array([-1.2512207e-03, -3.3569336e-04,  1.8310547e-04, ...,\n",
       "        -1.5157626e-06, -1.5157626e-06, -1.5157626e-06], dtype=float32),\n",
       " array([ 1.8615723e-03,  6.4086914e-04,  2.7465820e-04, ...,\n",
       "        -4.9510081e-06, -4.9510081e-06, -4.9510081e-06], dtype=float32),\n",
       " array([-3.1127930e-03, -1.4953613e-03, -1.2207031e-04, ...,\n",
       "        -1.0176266e-05, -1.0176266e-05, -1.0176266e-05], dtype=float32),\n",
       " array([-1.3916016e-02, -4.8828125e-03,  3.0822754e-03, ...,\n",
       "        -6.3601651e-06, -6.3601651e-06, -6.3601651e-06], dtype=float32),\n",
       " array([-1.0009766e-02, -1.4648438e-03,  2.7160645e-03, ...,\n",
       "         5.2975383e-06,  5.2975383e-06,  5.2975383e-06], dtype=float32),\n",
       " array([-1.3397217e-02, -1.1260986e-02, -1.3153076e-02, ...,\n",
       "        -2.4459216e-05, -2.4459216e-05, -2.4459216e-05], dtype=float32),\n",
       " array([ 1.8768311e-02,  1.7761230e-02,  1.6723633e-02, ...,\n",
       "        -9.2517718e-07, -9.2517718e-07, -9.2517718e-07], dtype=float32),\n",
       " array([-3.6926270e-03, -3.5400391e-03, -3.1433105e-03, ...,\n",
       "        -1.5797592e-05, -1.5797592e-05, -1.5797592e-05], dtype=float32),\n",
       " array([ 1.1108398e-02,  9.9487305e-03,  8.7890625e-03, ...,\n",
       "        -1.7691349e-08, -1.7691349e-08, -1.7691349e-08], dtype=float32),\n",
       " array([-3.2653809e-03, -2.6855469e-03, -4.2724609e-03, ...,\n",
       "        -4.3043747e-06, -4.3043747e-06, -4.3043747e-06], dtype=float32),\n",
       " array([-5.1574707e-03,  2.4108887e-03,  4.8828125e-04, ...,\n",
       "        -9.5236574e-06, -9.5236574e-06, -9.5236574e-06], dtype=float32),\n",
       " array([ 2.2277832e-03,  7.9345703e-03, -4.2724609e-03, ...,\n",
       "        -6.2311651e-06, -6.2311651e-06, -6.2311651e-06], dtype=float32),\n",
       " array([-1.8310547e-04, -2.1362305e-04, -4.8828125e-04, ...,\n",
       "         3.9918959e-06,  3.9918959e-06,  3.9918959e-06], dtype=float32),\n",
       " array([-2.1972656e-03, -3.2958984e-03, -4.6691895e-03, ...,\n",
       "         3.6704932e-06,  3.6704932e-06,  3.6704932e-06], dtype=float32),\n",
       " array([ 1.5258789e-04,  3.0517578e-05,  2.4414062e-04, ...,\n",
       "        -4.8409656e-06, -4.8409656e-06, -4.8409656e-06], dtype=float32),\n",
       " array([ 1.2207031e-04,  3.0517578e-05,  5.7983398e-04, ...,\n",
       "        -1.0367760e-05, -1.0367760e-05, -1.0367760e-05], dtype=float32),\n",
       " array([ 6.4086914e-04,  1.2207031e-04, -1.0070801e-03, ...,\n",
       "        -2.2570635e-05, -2.2570635e-05, -2.2570635e-05], dtype=float32),\n",
       " array([-1.5991211e-02, -1.5167236e-02, -1.3977051e-02, ...,\n",
       "        -3.4293482e-06, -3.4293482e-06, -3.4293482e-06], dtype=float32),\n",
       " array([-3.3569336e-04, -4.2724609e-04, -4.8828125e-04, ...,\n",
       "        -7.2995858e-06, -7.2995858e-06, -7.2995858e-06], dtype=float32),\n",
       " array([-2.0141602e-03,  2.0141602e-03, -6.4086914e-04, ...,\n",
       "         1.6348702e-08,  1.6348702e-08,  1.6348702e-08], dtype=float32),\n",
       " array([ 2.94189453e-02,  1.38549805e-02, -2.04467773e-02, ...,\n",
       "        -6.93855691e-06, -6.93855691e-06, -6.93855691e-06], dtype=float32),\n",
       " array([ 2.8991699e-03,  2.8686523e-03,  2.8686523e-03, ...,\n",
       "        -4.7291865e-06, -4.7291865e-06, -4.7291865e-06], dtype=float32),\n",
       " array([-1.4617920e-02, -2.2277832e-03, -8.5144043e-03, ...,\n",
       "        -2.9124385e-06, -2.9124385e-06, -2.9124385e-06], dtype=float32),\n",
       " array([ 4.6997070e-03,  8.3312988e-03,  3.5400391e-03, ...,\n",
       "        -1.1068176e-05, -1.1068176e-05, -1.1068176e-05], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaning the audio files.....\")\n",
    "train_wavs = [clean_single_wav(wav) for wav in train_wavs]\n",
    "print(\"Audio files cleaned \\u2705 \\u2705 \\u2705 \\u2705\\n\")\n",
    "train_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c55077",
   "metadata": {},
   "source": [
    "# Generate mfcc features for the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e852b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mfcc features.....\n",
      "MFCC features generated ✅ ✅ ✅ ✅\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-4.796408  ,  1.5434637 ,  0.636496  , ..., -0.09425944,\n",
       "         0.34669274,  0.33026385], dtype=float32),\n",
       " array([-3.1955981 ,  1.217868  ,  0.621759  , ...,  0.6174974 ,\n",
       "         0.17604741, -0.05681407], dtype=float32),\n",
       " array([-3.1990135 ,  1.1079593 ,  0.6238055 , ..., -0.04251979,\n",
       "         0.11003518,  0.09648506], dtype=float32),\n",
       " array([-3.7469773 ,  1.7098516 ,  0.46088555, ...,  0.29264623,\n",
       "         0.252809  ,  0.3118067 ], dtype=float32),\n",
       " array([-3.8844001 ,  2.0760522 ,  0.7067117 , ...,  0.27621436,\n",
       "         0.13850644,  0.32015398], dtype=float32),\n",
       " array([-3.2187698 ,  1.1329865 ,  0.6421117 , ...,  0.08824953,\n",
       "         0.26802835,  0.27487585], dtype=float32),\n",
       " array([-3.6637573 ,  1.3264847 ,  0.9430076 , ...,  0.11223838,\n",
       "         0.23151182,  0.13336673], dtype=float32),\n",
       " array([-4.0937696 , -0.17164868,  1.0485486 , ...,  0.19568054,\n",
       "         0.44634715,  0.10186896], dtype=float32),\n",
       " array([-2.822823  ,  0.6948634 ,  1.493339  , ...,  0.2614791 ,\n",
       "         0.2706865 ,  0.32897028], dtype=float32),\n",
       " array([-2.624257  ,  1.9023324 ,  0.20730565, ...,  0.0564819 ,\n",
       "         0.41605172, -0.09221944], dtype=float32),\n",
       " array([-2.6703143 ,  1.3824004 ,  0.20900518, ...,  0.16245544,\n",
       "         0.33047944,  0.14983678], dtype=float32),\n",
       " array([-3.643052  ,  1.5292543 ,  0.7178931 , ...,  0.1760489 ,\n",
       "         0.13303268,  0.09710132], dtype=float32),\n",
       " array([-3.3171606 ,  1.4855875 ,  0.22572346, ...,  0.08386363,\n",
       "         0.24397331,  0.18269609], dtype=float32),\n",
       " array([-3.4078481 ,  1.3713253 ,  0.15598074, ...,  0.17126766,\n",
       "         0.4148221 ,  0.12439705], dtype=float32),\n",
       " array([-3.272587  ,  0.87285274,  0.28925225, ...,  0.13163953,\n",
       "         0.40336055,  0.2036374 ], dtype=float32),\n",
       " array([-3.9085612 , -0.92177117,  0.94004923, ...,  0.3818596 ,\n",
       "         0.21211785,  0.09689856], dtype=float32),\n",
       " array([-5.2842765 ,  1.8284373 ,  0.7925528 , ...,  0.04542205,\n",
       "         0.11547901,  0.170975  ], dtype=float32),\n",
       " array([-4.1152143 , -0.27704653,  1.0643649 , ...,  0.15545031,\n",
       "         0.24800071,  0.32076475], dtype=float32),\n",
       " array([-1.8837454 ,  2.0661426 , -0.7657657 , ...,  0.19430591,\n",
       "         0.22065146,  0.2044443 ], dtype=float32),\n",
       " array([-3.5650277 ,  0.8489768 ,  0.5147963 , ...,  0.0103938 ,\n",
       "         0.20322834,  0.04185473], dtype=float32),\n",
       " array([-2.958249  ,  1.7135895 ,  0.6984534 , ...,  0.26260608,\n",
       "         0.3422515 ,  0.16850208], dtype=float32),\n",
       " array([-3.3826237 ,  0.9767532 ,  1.0285432 , ...,  0.18135442,\n",
       "         0.21220922,  0.14308609], dtype=float32),\n",
       " array([-3.8761647 ,  1.4757751 ,  0.6953011 , ...,  0.33698648,\n",
       "         0.08674362,  0.42150825], dtype=float32),\n",
       " array([-3.330508  ,  1.6686226 ,  0.266409  , ...,  0.20036806,\n",
       "         0.38539255,  0.279857  ], dtype=float32),\n",
       " array([-3.9776378 ,  1.6204625 ,  0.61070347, ...,  0.3181262 ,\n",
       "         0.31530425,  0.27286518], dtype=float32),\n",
       " array([-2.642243  ,  1.3243577 ,  0.12829354, ...,  0.25845015,\n",
       "         0.23060341,  0.2577417 ], dtype=float32),\n",
       " array([-2.7378657 ,  1.6622255 ,  0.21466917, ...,  0.09300359,\n",
       "         0.11740318,  0.15738793], dtype=float32),\n",
       " array([-2.9584703 , -0.06555403,  0.71385103, ...,  0.11916407,\n",
       "         0.17515923,  0.10596479], dtype=float32),\n",
       " array([-4.1774707 ,  2.3830671 ,  0.31827366, ...,  0.20989485,\n",
       "         0.34118506,  0.14155757], dtype=float32),\n",
       " array([-4.069203  ,  1.4360524 ,  0.77579206, ..., -0.07525378,\n",
       "         0.12949751,  0.24464609], dtype=float32),\n",
       " array([-3.611545  ,  1.915321  ,  0.11304256, ...,  0.12153777,\n",
       "         0.20780942,  0.38169813], dtype=float32),\n",
       " array([-4.676533  ,  1.3451722 ,  0.58484244, ...,  0.08221464,\n",
       "         0.22618905,  0.16781937], dtype=float32),\n",
       " array([-4.5864897 ,  2.3208292 ,  0.88161165, ...,  0.18247642,\n",
       "         0.25182092,  0.04873684], dtype=float32),\n",
       " array([-3.5841897 ,  0.82740134,  0.8046005 , ...,  0.257508  ,\n",
       "         0.12021556, -0.07458424], dtype=float32),\n",
       " array([-4.194393  ,  1.3869642 ,  0.73607355, ...,  0.11896221,\n",
       "         0.14819165,  0.14359881], dtype=float32),\n",
       " array([-4.356793  , -0.28607622,  0.8401378 , ...,  0.3024898 ,\n",
       "         0.09577537,  0.05414342], dtype=float32),\n",
       " array([-2.4344363 ,  1.763116  ,  0.21782984, ...,  0.21194772,\n",
       "         0.32434633,  0.25852042], dtype=float32),\n",
       " array([-3.9597592 ,  1.3120984 ,  0.44778335, ...,  0.11155485,\n",
       "         0.26063004,  0.2532691 ], dtype=float32),\n",
       " array([-3.2289412 ,  1.3191748 ,  0.35976663, ..., -0.02250664,\n",
       "        -0.10933948,  0.18809257], dtype=float32),\n",
       " array([-2.9829638 ,  0.48964104,  0.70243335, ...,  0.31670836,\n",
       "         0.3313113 ,  0.05615155], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generating mfcc features.....\")\n",
    "train_wavs = [gen_mfcc(wav) for wav in train_wavs]\n",
    "print(\"MFCC features generated \\u2705 \\u2705 \\u2705 \\u2705\\n\")\n",
    "train_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a30a62",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901eb34d",
   "metadata": {},
   "source": [
    "### Originally the data was split in the 95% train and 5% test set; With total of 148K (audio,text) pairs.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ace914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wavs, test_wavs, train_texts, test_texts = train_test_split(train_wavs,\n",
    "                                                                  train_texts,\n",
    "                                                                  test_size=0.2)\n",
    "#train_wavs\n",
    "#test_wavs\n",
    "#train_texts\n",
    "#test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beddf4",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "### Originally the model was trained for 58 epochs; With a batch size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f64cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000209AE0AC4C0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py\", line 5160, in <genexpr>\n",
      "    output_ta_t = tuple(  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [02:10<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:14<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 65.54, Test Loss 86.34, Test CER 95.25 % in 144.76 secs.\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [02:16<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:17<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 65.29, Test Loss 84.54, Test CER 93.46 % in 154.81 secs.\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, \n",
    "            optimizer, \n",
    "            train_wavs, \n",
    "            train_texts,\n",
    "            test_wavs, \n",
    "            test_texts, \n",
    "            epochs=10, \n",
    "            batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c7162",
   "metadata": {},
   "source": [
    "# Save the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b798a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/trained_model_v1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# model.save(\"model/trained_model_v1.h5\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(model,\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model/trained_model_v1.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/trained_model_v1.pkl'"
     ]
    }
   ],
   "source": [
    "# model.save(\"model/trained_model_v1.h5\")\n",
    "import pickle\n",
    "pickle.dump(model,open('./model/trained_model_v1.pkl'))\n",
    "# Not working to save the error file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09f329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
