{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1662801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import edit_distance as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe466e",
   "metadata": {},
   "source": [
    "# Global configs required for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07795c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run configs.ipynb\n",
    "device_name = '/device:CPU:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b0fe7",
   "metadata": {},
   "source": [
    "# U.0\n",
    "# Loads model from the directory argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8efc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "    return models.load_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8670c",
   "metadata": {},
   "source": [
    "# U.1\n",
    "# Loading wav file from librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad37b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(dir):\n",
    "    return librosa.load(dir, sr=SR)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ff54c",
   "metadata": {},
   "source": [
    "# U.2\n",
    "# Generates Normalized MFCCs from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70f7de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mfcc(arr):\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=arr[:-1], sr=SR, n_mfcc=N_MFCC, hop_length=HOP_LENGTH).transpose().flatten()\n",
    "    return (mfccs - np.mean(mfccs)) / np.std(mfccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec5554",
   "metadata": {},
   "source": [
    "# U.3\n",
    "# Generates padded text from list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd4374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(list_texts, unq_chars, unk_idx=1):\n",
    "    max_len = max([len(txt) for txt in list_texts])\n",
    "    padded_arr = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    for txt in list_texts:\n",
    "        len_seq = len(txt)\n",
    "        txt += \"0\" * (max_len - len_seq)\n",
    "\n",
    "        # index 1 for the unknown chars\n",
    "        arr = np.array(\n",
    "            [unq_chars.index(ch) if ch in unq_chars else unk_idx for ch in txt])\n",
    "\n",
    "        padded_arr.append(arr)\n",
    "        seq_lengths.append(len_seq)\n",
    "\n",
    "    return np.array(padded_arr), np.array(seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237615a",
   "metadata": {},
   "source": [
    "# U.4\n",
    "# Returns tensor batch*seq*frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba48ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_np(list_np):\n",
    "    max_len = max([len(arr) for arr in list_np])\n",
    "\n",
    "    # So that the numpy array can be reshaped according to the input dimension\n",
    "    max_len += INPUT_DIM - (max_len % INPUT_DIM)\n",
    "\n",
    "    padded_arr = []\n",
    "\n",
    "    for arr in list_np:\n",
    "        len_seq = len(arr)\n",
    "        arr = np.pad(arr, (0, max_len - len_seq), constant_values=0)\n",
    "        padded_arr.append(arr)\n",
    "\n",
    "    return np.array(padded_arr).reshape((len(list_np), -1, INPUT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d61ab1",
   "metadata": {},
   "source": [
    "# U.5\n",
    "# Generates batches of wavs and texts  with padding as per needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee3f1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(wavs, texts, unq_chars):\n",
    "    assert len(wavs) == len(texts)\n",
    "    # generates tensor of dim (batch * seq * frame)\n",
    "    input_tensor = pad_list_np(wavs)\n",
    "    target_tensor, target_lengths_tensor = pad_text(texts, unq_chars)\n",
    "    output_seq_lengths_tensor = np.full(\n",
    "        (len(wavs),), fill_value=input_tensor.shape[1])\n",
    "\n",
    "    return input_tensor, target_tensor, target_lengths_tensor.reshape((-1, 1)), output_seq_lengths_tensor.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36632fb9",
   "metadata": {},
   "source": [
    "# U.6\n",
    "# Plots lossses from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbbb3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(dir, optimal_epoch=None):\n",
    "    losses = None\n",
    "    with open(dir, \"rb\") as f:\n",
    "        losses = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    train_losses, test_losses = losses[\"train_losses\"], losses[\"test_losses\"]\n",
    "    epochs = len(train_losses)\n",
    "    # print(len(test_losses))\n",
    "\n",
    "    X = range(1, epochs+1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "\n",
    "    fig.suptitle('Train and Test Losses', fontsize=25,)\n",
    "\n",
    "    ax.set_xlim(0, 72)\n",
    "    ax.plot(X, train_losses, color=\"red\", label=\"Train Loss\")\n",
    "    ax.plot(X, test_losses, color=\"green\", label=\"Test Loss\")\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "    plt.legend(loc=\"upper right\", frameon=False, fontsize=20)\n",
    "    # plt.xlabel(\"Epochs\",{\"size\":20})\n",
    "    # plt.ylabel(\"Loss\", {\"size\":20})\n",
    "\n",
    "    if (optimal_epoch != None):\n",
    "        plt.axvline(x=optimal_epoch, ymax=0.5)\n",
    "        # ax.plot(58, 0, 'go', label='marker only')\n",
    "        plt.text(optimal_epoch, 35,\n",
    "                 f'Optimal Epoch at {optimal_epoch}', fontsize=15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa5e9a",
   "metadata": {},
   "source": [
    "# U.7\n",
    "# Decoding with prefix beam search from MFCC features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32901c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_mfccs(model, mfccs, unq_chars):\n",
    "\n",
    "    mfccs = pad_list_np(mfccs)  # coverts the data to 3d\n",
    "    pred = model(mfccs, training=False)\n",
    "\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = K.ctc_decode(pred, input_length=input_len,\n",
    "                           greedy=False, beam_width=100)[0][0]\n",
    "\n",
    "    sentences = []\n",
    "    char_indices = []\n",
    "    for chars_indices in results:\n",
    "\n",
    "        sent = \"\"\n",
    "        temp_indices = []\n",
    "        for idx in chars_indices:\n",
    "\n",
    "            if idx > 1:\n",
    "                sent += unq_chars[idx]\n",
    "                temp_indices.append(idx.numpy())\n",
    "        sentences.append(sent)\n",
    "        char_indices.append(temp_indices)\n",
    "    return sentences, char_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220fa3e",
   "metadata": {},
   "source": [
    "# U.8\n",
    "# Decoding with prefix beam search from wavs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faf5831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_wavs(model, wavs, unq_chars):\n",
    "    mfccs = [gen_mfcc(wav) for wav in wavs]\n",
    "    return predict_from_mfccs(model, mfccs, unq_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fb6c4",
   "metadata": {},
   "source": [
    "# U.9\n",
    "# Converts the text to list of indices as per the unique characters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51bda869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_from_texts(texts_list, unq_chars, unk_idx=1):\n",
    "\n",
    "    indices_list = []\n",
    "    for txt in texts_list:\n",
    "\n",
    "        # index 1 for the unknown chars\n",
    "        lst = [unq_chars.index(\n",
    "            ch) if ch in unq_chars else unk_idx for ch in txt]\n",
    "\n",
    "        indices_list.append(lst)\n",
    "\n",
    "    return indices_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8fcee",
   "metadata": {},
   "source": [
    "'''\n",
    "Calculates CER( character error rate) from dataset;\n",
    "'''\n",
    "# U.10\n",
    "# CER from mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0123d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CER_from_mfccs(model, mfccs, texts, unq_chars, batch_size=100):\n",
    "\n",
    "    with tf.device(device_name):\n",
    "\n",
    "        len_mfccs = len(mfccs)\n",
    "        batch_count = 0\n",
    "        sum_cer = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for start in range(0, len_mfccs, batch_size):\n",
    "            end = None\n",
    "            if start + batch_size < len_mfccs:\n",
    "                end = start + batch_size\n",
    "            else:\n",
    "                end = len_mfccs\n",
    "            pred_sentences, pred_indices = predict_from_mfccs(\n",
    "                model, mfccs[start:end], unq_chars)\n",
    "            actual_indices = indices_from_texts(texts[start:end], unq_chars)\n",
    "\n",
    "            len_batch_texts = end - start\n",
    "            batch_cer = 0\n",
    "            for i in range(len_batch_texts):\n",
    "\n",
    "                pred = pred_indices[i]\n",
    "                actu = actual_indices[i]\n",
    "\n",
    "                sm = ed.SequenceMatcher(pred, actu)\n",
    "                ed_dist = sm.distance()\n",
    "                batch_cer += ed_dist / len(actu)\n",
    "\n",
    "            batch_cer /= len_batch_texts\n",
    "            batch_count += 1\n",
    "            sum_cer += batch_cer\n",
    "\n",
    "            print(\"CER -> {:.2f}%, \\t No.of sentences -> {}, \\t Time Taken -> {:.2f} secs.\".format(\n",
    "                (sum_cer / batch_count) * 100, end, time.time() - start_time))\n",
    "\n",
    "        print(\n",
    "            \"The total time taken for all sentences CER calculation is  {:.2f} secs.\".format(time.time() - start_time))\n",
    "        return sum_cer / batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f5e35",
   "metadata": {},
   "source": [
    "# U.11\n",
    "# CER from wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e62eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CER_from_wavs(model, wavs, texts, unq_chars, batch_size=100):\n",
    "\n",
    "    assert len(wavs) == len(texts)\n",
    "\n",
    "    len_wavs = len(wavs)\n",
    "    for i in range(len_wavs):\n",
    "        wavs[i] = gen_mfcc(wavs[i])\n",
    "\n",
    "    return CER_from_mfccs(model, wavs, texts, unq_chars, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c7da4",
   "metadata": {},
   "source": [
    "# U.12\n",
    "# CTC softmax probabilities output from mfcc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09d75254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_softmax_output_from_mfccs(model, mfccs):\n",
    "    mfccs = pad_list_np(mfccs)\n",
    "    y = model(mfccs)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a798d",
   "metadata": {},
   "source": [
    "# U.13\n",
    "# CTC softmax probabilities output from wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0742aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_softmax_output_from_wavs(model, wavs):\n",
    "    mfccs = [gen_mfcc(wav) for wav in wavs]\n",
    "    return ctc_softmax_output_from_mfccs(model, mfccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a3c14",
   "metadata": {},
   "source": [
    "# U.14\n",
    "# Clean the single audio file by clipping silent gaps from both ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a455b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_single_wav(wav, win_size=500):\n",
    "    wav_avg = np.average(np.absolute(wav))\n",
    "\n",
    "    for s in range(0, len(wav)-win_size, win_size):\n",
    "        window = wav[s:s+win_size]\n",
    "        if np.average(np.absolute(window)) > wav_avg:\n",
    "            wav = wav[s:]\n",
    "            break\n",
    "\n",
    "    for s in range(len(wav)-win_size, 0, -win_size):\n",
    "        window = wav[s-win_size:s]\n",
    "        if np.average(np.absolute(window)) > wav_avg:\n",
    "            wav = wav[:s]\n",
    "            break\n",
    "\n",
    "    pad = FRAME_SIZE - len(wav) % FRAME_SIZE\n",
    "    wav = np.pad(wav, (0, pad), mode=\"mean\")\n",
    "    return wav"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
